---
title: "Auto-Complete Tool for Yelp Reviews"
author: "Raveena Vemula and Sanjitha Udipi"
date: "December 12, 2016"
output: pdf_document
---

```{r setup, include=FALSE}
options(warn=-1)
knitr::opts_chunk$set(echo = TRUE)
setwd("C:/Users/SANYA/Desktop/Auto-Complete Tool for Yelp Reviews")
#install.packages("tm")
#install.packages("SnowballC")
#install.packages("RWeka")
#install.packages("readr")
#install.packages("plyr")
#install.packages("RWeka")
#install.packages("wordcloud") 
#install.packages("RColorBrewer") 
#install.packages("ggplot2")
#install.packages("topicmodels")
#install.packages("RTextTools")
#install.packages("topicmodels")
#install.packages("xtable")
#install.packages("plotly")
#install.packages("strtrim")
#install.packages("stringr", dependencies=TRUE)
#install.packages("stringi", dependencies=TRUE)
#install.packages("qdap")
#install.packages("data.table")
#install.packages("lsa")
#install.packages("trimws")
#install.packages("clusteval")
library(clusteval)
library(xtable)
library(lsa)
library(RTextTools)
library(topicmodels)
library(readr)
library(tm)
library(SnowballC)
library(RWeka)
library(ggplot2)
library(plyr)
library(RWeka)
library(wordcloud)
library(RColorBrewer)
library(ggplot2)
library(topicmodels)
library(plotly)
library(stringr)
library(stringi)
#library(strtrim)
library(qdap)
library(data.table)
```

## List of functions used:

#### <br/><br/>Function for stemming
```{r}
stemming <- function(text) {
  #Takes a vector of reviews from businessUserData
  #
  #Args: 
  # text: vector of reviews
  #Returns:
  # afterStemText: vector of reviews after stemming
  
  #vector of words
  List <- strsplit(text, " ")
  Words=unlist(List)
  afterStemText = c()
  #stemming on all reviews
  for(k in 1:length(text)){
    #remove special characters and split the string
    businessUserText <- gsub("\"", "", trim(text[k]))
    businessUserText <- gsub("\\(|\\)", "", businessUserText)
    businessUserVector <- strsplit(businessUserText, " ")
    businessUserWords = unlist(businessUserVector)
    #stemming
    stemDocCorpus <- stemDocument(businessUserWords)
    #stem completion
    stemCompleteDocCorpus <-  stemCompletion(stemDocCorpus, dictionary=Words)
    
    f=c()
    stemText=c()
    for(i in 1:length(stemCompleteDocCorpus)){
      f = c(f,stemCompleteDocCorpus[[i]])
      stemText[i] = paste(f, collapse=" ")
    }
    # Review stored in vector after stemming
    afterStemText[k] = stemText[length(stemText)]
  }
  return(afterStemText)
}
```

#### <br/><br/>Function to generate TermDocumentMatrix
```{r}
generateTermDocumentMatrix <- function(reviews, ng) {
  #Takes a vector of reviews from businessUserData and n-gram size
  #
  #Args: 
  # reviews: vector of reviews (after stemming)
  # ng: n-gram size
  #Returns:
  # doctm: term document matrix for n-grams
  
  #create a corpus (a collection of documents)
  docCorpus <- Corpus(VectorSource(reviews))
  #remove url
  removeURL <- function(x) gsub('http.*\\s*', '', x)
  docCorpus <- tm_map(docCorpus, content_transformer(removeURL))
  #remove punctuation
  docCorpus <- tm_map(docCorpus, removePunctuation)
  #to lower case
  docCorpus<- tm_map(docCorpus, content_transformer(tolower))
  #remove numbers
  docCorpus <- tm_map(docCorpus, removeNumbers)
  #remove space
  docCorpus <- tm_map(docCorpus, stripWhitespace)
  
  options(mc.cores=1) 
  #prepare tokens of size 'n'
  ngramTokenizer <- function(x) RWeka::NGramTokenizer(x, Weka_control(min = ng, max = ng)) 
  #term document matrix for n-grams
  doctm <- TermDocumentMatrix(docCorpus, control = list(tokenize = ngramTokenizer)) 
  #return Term Document Matrix
  return(doctm)
}
```

#### <br/><br/>Function for term frequency-inverse document frequency
```{r}
calculateTfidf=function(mat){
  tf <- mat
  id=function(col){
    sum(!col==0)
  }
  idf <- log(nrow(mat)/apply(mat, 2, id))
  tfidf <- mat
  for(word in names(idf)){
    tfidf[,word] <- as.integer(tf[,word] * idf[word])
  }
  return(tfidf)
}
```

#### <br/><br/>Other functions:
```{r}
# Function to trim a string
trim <- function( x ) {
  #Takes a string
  #
  #Args: 
  # x: string
  #Returns:
  # string after trim
  
  gsub("(^[[:space:]]+|[[:space:]]+$)", "", x)
}

# Function to get last word of a sentence
lastWord <- function(sentence) {
  #Takes a sentence as input
  #
  #Args: 
  # sentence: string
  #Returns:
  # last word of the sentence
  
  return(word(sentence,-1))
}

# Function to return firstwords (all except last word)
firstWords <- function(sentence) {
  #Takes a sentence as input
  #
  #Args: 
  # sentence: string
  #Returns:
  # all word except the last word of the sentence as a string
  
  return(word(sentence,start=1,end=-2))
}
```

#### <br/><br/>Algorithm for Kneser-Ney Smoothing
```{r}
KNSmoothing = function(dt, n){
    # Takes a data table and n-gram size as input
    #
    # Args: 
    #    dt: a data table with term, nextTerm and frequency
    #    n:  the n-gram size 
    # Returns:
    #    dt: data table with smoothed probabilities
  
  col <- colnames(dt)
  col <- col[1:n]
  
  
  # Y = N_c / (N_c + 2N_(c+1)), where N_c is the count of ngrams with count==c (for Kneser-Ney)
  #     Note: This is a simplified calculation of Y using only the two lowest kept freqs.
  c1 <- min(dt$Freq) # The lowest available Freq
  c2 <- min(subset(dt, Freq>c1)$Freq) # And the second lowest
  Y <- nrow(dt[Freq == c1]) / (nrow(dt[Freq == c1]) + 2 * nrow(dt[Freq == c2])) # 1:0.50 2:0.56 3+:0.62  
  
  # D = Discounting parameter different for freq==1, freq==2 and freq>=3
  #     Ref Goodman and Chen (1999)
  dt[, D := 0]
  dt[Freq == 1]$D <- 1 - 2 * Y * (nrow(dt[Freq == 2]) / nrow(dt[Freq == 1]))
  dt[Freq == 2]$D <- 2 - 3 * Y * (nrow(dt[Freq == 3]) / nrow(dt[Freq == 2]))
  dt[Freq > 2]$D  <- 3 - 4 * Y * (nrow(dt[Freq == 4]) / nrow(dt[Freq == 3]))
  
  # Nom = First nominator in P_KN formula ( max{c(w_i-1, w_i)-D, 0} )
  dt <- dt[, Nom := pmax(Freq-D, 0)]
  
  # Denom = Denominator is the count of the preceding word(s) 
  if(n==1) {
    dt <- dt[, Denom := sum(Freq)]
  } else if (n==2) {
    dt <- dt[, .(nextWord, Freq, D, Nom, Denom = sum(Freq)), by = word1]
  } else if (n==3) {
    dt <- dt[, .(nextWord, Freq, D, Nom, Denom = sum(Freq)), by = list(word2, word1)]
  } else if (n==4) {
    dt <- dt[, .(nextWord, Freq, D, Nom, Denom = sum(Freq)), by = list(word3, word2, word1)]
  }
  
  
  # NN = number of word types that follows w_i-1 in the training data
  if(n==1) {
    dt <- dt[, .(nextWord, Freq, D, Nom, Denom, NN = length(nextWord))]
  } else if (n==2) {
    dt <- dt[, .(nextWord, Freq, D, Nom, Denom, NN = length(nextWord)), by=word1]
  } else if (n==3) {
    dt <- dt[, .(nextWord, Freq, D, Nom, Denom, NN = length(nextWord)), by=list(word2, word1)]
  } else if (n==4) {
    dt <- dt[, .(nextWord, Freq, D, Nom, Denom, NN = length(nextWord)), by=list(word3, word2, word1)]
  }
  
  
  # L  = Lambda, normalizing constant, the probability mass we've discounted
  dt[, L := (D / Freq) * NN]
  
  # N = The number of different ngrams this nextword completes in training set 
  #     (c(w_(i-1)) for Kneser-Ney). Used in P_continutation (PC)
  if(n==1) {
    dt <- dt[, .(nextWord, Freq, D, Nom, Denom, NN, L, .N)]
  } else if (n==2) {
    dt <- dt[, .(word1, Freq, D, Nom, Denom, NN, L, .N), by=nextWord]
  } else if (n==3) {
    dt <- dt[, .(word2, word1, Freq, D, Nom, Denom, NN, L, .N), by=nextWord]
  } else if (n==4) {
    dt <- dt[, .(word3, word2, word1, Freq, D, Nom, Denom, NN, L, .N), by=nextWord]
  }
  
  
  # PC = P_continuation
  dt[, PC := N / nrow(dt)] # Count of this novel continuation div. by number of unique grams
  
  # Prob_KN - Estimated KN probability
  dt[, P_KN := (Nom/Denom) + ((D/Denom) * NN) * PC]
  
  all_freq <- sum(dt$Freq)
  dt[, leftOver:=(1-sum((((Nom/Denom) + ((D/Denom) * NN) * PC)*Freq)/all_freq))]
  
  return(dt)
}
```

#### <br/><br/>Function generateWordCloud to generate the word cloud for the n-grams produced
```{r}
generateWordCloud <- function(nGramReviewMatrix, freqValue) {
  #Takes a Term Document Matrix, minimum frequency value for wordcloud
  #
  #Args: 
  # nGramReviewMatrix: n-gram term document matrix
  # freqValue: minimum frequency value
  #Returns:
  # wordcloud for n-grams
  
  freq <- sort(colSums(nGramReviewMatrix), decreasing=TRUE) 
  set.seed(123)   
  # Minimum frequency is set to 2
  wordcloud(names(freq),freq,min.freq=freqValue,colors=brewer.pal(6,"Dark2"))
}
```


#### <br/><br/> Function to generate plot for top 10 n-grams
```{r}
generateDFPlot = function(TdmTfidf, yaxis, title){
  #Takes a Term Document Matrix, yaxis label and plot title
  #
  #Args: 
  # TdmTfidf: term document matrix
  # yaxis: Y-axis label
  # title: title for plot
  #Returns:
  # plot for top 10 n-grams
  
  #plot top 10 terms and their frequency
  colSumMatrix <- colSums(TdmTfidf)
  NGramsList <- names(colSumMatrix)
  NGramsFreq <- as.numeric(colSumMatrix)
  NGramFreqDF <- data.frame(NGramsList, NGramsFreq)
  NGramFreqOrdDF <- NGramFreqDF[order(NGramFreqDF$NGramsFreq, decreasing = TRUE),]
  plotNGram <- NGramFreqOrdDF[1:10,]
  names(plotNGram) <- c("NGram", "Frequency")
  barplot(plotNGram$Frequency, names = plotNGram$NGram,
          xlab = "Frequency", ylab = yaxis,horiz=TRUE,
          main = title, col=c("lightblue"))
}
```

#### <br/><br/>Topic modelling using LDA
```{r}
generateTopicModellingUsingLDA <- function (nGramMatrix,k) {
  #Takes a Term Document Matrix and number of topics
  #
  #Args: 
  # nGramMatrix: n-gram term document matrix
  # k: number of topics
  #Returns:
  # LDA topics and terms
  
  #Set parameters for Gibbs sampling
  burnin <- 4000
  iter <- 2000
  thin <- 500
  seed <-list(2003,5,63,100001,765)
  nstart <- 5
  best <- TRUE
  k <- k
  #Run LDA using Gibbs sampling
  ldaOut <-LDA(nGramMatrix,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
  #Topics assigned to each word
  ldaOut.topics <- as.matrix(topics(ldaOut))
  
  return(ldaOut)
}
```

#### <br/><br/>Function to generate top N terms in a topic
```{r}
generateTopNTermsInTopic <- function(ldaModel,n) {
  #Takes an LDA model and parameter 'n'
  #
  #Args: 
  # ldaModel: LDA output
  # n: number of terms
  #Returns:
  # matrix with top N terms in each topic
  
  as.matrix(terms(ldaModel,n))
}
```

#### <br/><br/>Functions related to topic and term probabilities
```{r}
# Function to calculate probability of each topic in each document
showTopicProbabilities <- function(ldaModel) { 
  #Takes an LDA model as input
  #
  #Args: 
  # ldaModel: LDA output
  #Returns:
  # probability for each topic vs document
  
  #probabilities associated with each topic assignment for a document
  topicProbabilities <- as.data.frame(ldaModel)
  names(topicProbabilities) <- c(1:10)
  head(topicProbabilities, 10)
}

# Function to calculate probability of each topic across all documents
calculateTopicProbabilityAcrossAllDocuments <- function(ldaModel) {
  #Takes an LDA model as input
  #
  #Args: 
  # ldaModel: LDA output
  #Returns:
  # probability for each topic across all documents
  
  #Computing the mean ocuurence of each topic across documents and sorting it in decreasing order
  topicProbabilities <- as.data.frame(ldaModel)
  names(topicProbabilities) <- c(1:10)
  
  probab = c()
  numb = c()
  for(i in 1:10){
    numb[i] = i
    probab[i] = mean(topicProbabilities[,i])
  }
  meanProbabDF <- data.frame(numb, probab)
  meanOrderedProbabDF <- meanProbabDF[order(meanProbabDF$probab, decreasing = TRUE),]
  names(meanOrderedProbabDF) <- c("Topic", "Probability")
barplot(meanOrderedProbabDF$Probability, names=meanOrderedProbabDF$Topic, xlab="Topic", ylab="Probability", main="Topic vs Probability", col=c("lightblue"))
  return(meanOrderedProbabDF)
}

# Assign each term the probability of its LDA topic
assignTermLdaProbability <- function(ldaModel,meanOrderedProbabDF) {
  #Takes an LDA model and probability of topics across all documents (as data frame)
  #
  #Args: 
  # ldaModel: LDA output
  # meanOrderedProbabDF: probability of each topic across all documents
  #Returns:
  # each term with LDA probability
  
  topTermsPerTopic <- as.data.frame(terms(ldaModel,40))
  names(topTermsPerTopic) <- c(1:10)
  
  #Order the terms according to topic probability
  oldDT <- data.table()
  for(i in 1:nrow(meanOrderedProbabDF)) {
    row <- meanOrderedProbabDF[i,]
    topicTerms <- as.vector(topTermsPerTopic[[row$Topic]])
    for(term in topicTerms) {
      newDT <- data.table(terms=term,ldaProbability=(row$Probability))
      if(nrow(oldDT) == 0) {
        oldDT <- rbind.fill(oldDT,newDT)
      } else {
        duplicateTerms <- subset(oldDT,terms == term)
        if(nrow(duplicateTerms) == 0) {
          oldDT <- rbind.fill(oldDT,newDT)
        }
      }
    }
  }
  return(oldDT)
}

# Function to calculate mean probability of LDA and Back-off Model
mergeProbability <- function(ldaDT,smoothedDT) {
  #Takes LDA and KN smoothing output as inputs
  #
  #Args: 
  # ldaDT: LDA output 
  # smoothedDT: KN smoothing output
  #Returns:
  # mean of LDA and Back-off probability for each term
  
  if("nextTerm" %in% colnames(smoothedDT))
  {
    smoothedDT$wholeTerm = paste(smoothedDT$term, smoothedDT$nextTerm, sep=" ")
    
    # merge the two data frames based on terms in lda and whole terms in back-off
    setkey(as.data.table(ldaDT),terms)
    setkey(smoothedDT,wholeTerm)
    
    # perform the join, eliminating not matched rows from Right
    combinedDT <- smoothedDT[ldaDT, nomatch=0]
  }  else {
    # merge the two data frames based on terms in lda and whole terms in back-off
    setkey(as.data.table(ldaDT),terms)
    setkey(smoothedDT,term)
    
    # perform the join, eliminating not matched rows from Right
    combinedDT <- smoothedDT[ldaDT, nomatch=0]
  }
  
  combinedDT$mean=rowMeans(combinedDT[,c("probability", "ldaProbability")], na.rm=TRUE)
  return(combinedDT)
}
```

#### <br/><br/>Prediction Model (Katz Back-off Model)
```{r}
getLastTerms = function(inputString,num){
  # Takes a string and number of terms to be returned
  #
  # Args:
  # inputString: string
  # num: number of last terms to be returned
  # Returns:
  # prints last 'num' terms
  
  # Preprocessing
  inputString = gsub("[[:space:]]+", " ", str_trim(tolower(inputString)))
  
  words = unlist(strsplit(inputString, " "))
  from = length(words)-num+1
  to = length(words)
  tempWords = words[from:to]
  paste(tempWords, collapse=" ")
}

predictionGiven3Words = function(inputString){
  # Takes a string as input
  #
  # Args:
  # inputString: string
  # Returns:
  # matrix with term, nextTerm, frequency, smoothedProbability, wholeTerm, ldaProbability, probability
  
  # Preprocessing
  inputString = gsub("[[:space:]]+", " ", str_trim(tolower(inputString)))
  words = unlist(strsplit(inputString, " "))
  
  # If number of words is less than 3 then calls predictionGiven2Words
  if (length(words) < 3){
    return(predictionGiven2Words(inputString))
  }
  # Call to get last 3 terms if number of words is >=3
  inputSplit.last3Words <- getLastTerms(inputString,3)
  # Matches the words with quad-grams
  quadGram.termsMatch = quad.combinedProb[term == inputSplit.last3Words]
  # If there is a match
  if (nrow(quadGram.termsMatch) > 0){
    # returns a matrix with term, nextTerm, frequency of occurrence, smoothedProbability, wholeTerm, ldaProbability and probability
    return(quadGram.termsMatch[order(quadGram.termsMatch$probability, decreasing = TRUE),])
  }
  # If there is no match
  else {
    return(predictionGiven2Words(inputString))
  }
}

predictionGiven2Words = function(inputString){
   # Takes a string as input
  #
  # Args:
  # inputString: string
  # Returns:
  # matrix with term, nextTerm, frequency, smoothedProbability, wholeTerm, ldaProbability, probability
  
  # Preprocessing
  inputString = gsub("[[:space:]]+", " ", str_trim(tolower(inputString)))
  words = unlist(strsplit(inputString, " "))
  
  # If number of words is less than 2 then calls predictionGiven1Word
  if (length(words) < 2){
    predictionGiven1Word(inputString)
    return
  }
  # Call to get last 2 terms if number of words is >=2
  inputSplit.last2Words <- getLastTerms(inputString,2)
  # Matches the words with tri-grams
  triGram.termsMatch = tri.combinedProb[term == inputSplit.last2Words]
  # If there is a match
  if (nrow(triGram.termsMatch) > 0){
    # returns a matrix with term, nextTerm, frequency of occurrence, smoothedProbability, wholeTerm, ldaProbability and probability
    return(triGram.termsMatch[order(triGram.termsMatch$probability, decreasing = TRUE),])
  }
  # If there is no match
  else {
    return(predictionGiven1Word(inputString))
  }
}


predictionGiven1Word = function(inputString){
  # Takes a string as input
  #
  # Args:
  # inputString: string
  # Returns:
  # matrix with term, nextTerm, frequency, smoothedProbability, wholeTerm, ldaProbability, probability
  
  # Preprocessing
  inputString = gsub("[[:space:]]+", " ", str_trim(tolower(inputString)))
  words = unlist(strsplit(inputString, " "))
  
  # If number of words is less than 1 then calls predictionGiven0Word
  if (length(words) < 1){
    return(predictionGiven0Word(inputString))
  }
  # Call to get last term if number of words is >=1
  inputSplit.last1Words <- getLastTerms(inputString,1)
  # Matches the words with bi-grams
  biGram.termsMatch = bi.combinedProb[term == inputSplit.last1Words]
  # If there is a match
  if (nrow(biGram.termsMatch) > 0){
    # returns a matrix with term, nextTerm, frequency of occurrence, smoothedProbability, wholeTerm, ldaProbability and probability
    return(biGram.termsMatch[order(biGram.termsMatch$probability, decreasing = TRUE),])
  }
  # If there is no match
  else {
    return(predictionGiven0Word(inputString))
  }
}

predictionGiven0Word = function(inputString){
  # Takes a string as input
  #
  # Args:
  # inputString: string
  # Returns:
  # uni-grams sorted according to probability
  
  orderedOutput <- uni.combinedProb[order(uni.combinedProb$probability, decreasing = TRUE),]
  if (nrow(orderedOutput) > 0){
    return(orderedOutput)
  } else {
    stop(sprintf("[%s] has no predictions that we could offer.", inputString))
  }
}
```

## <br/><br/>Main Program:

#### <br/><br/>Read Data:
```{r}
# Read the processed file
data <- read_delim("processed.csv",delim= "~", col_names=c("date", "stars", "user_id", "review_id", "business_id", "text"))

# Get the total number of reviews for the restuarant Ihop (restaurant with highest number of reviews) and user with highest number of reviews
businessUserData <- data[which( data$business_id=="T1jYZFB_7cqdhuvzpxfFWQ" | data$user_id=="q200TCFOheJfS_MvvwMfJw" ),]
```

#### <br/><br/>Division of data into training and test sets
```{r}
# 90% training set and 10% test set
trainRows <- ceiling(0.9*nrow(businessUserData))
trainData <- businessUserData[1:trainRows,]
testData <- businessUserData[(trainRows+1):nrow(businessUserData),]
```

#### <br/><br/>Stemming
```{r}
stemmedTrainData <- stemming(trainData$text)
```

#### <br/><br/>Steps to generate term document matrix, tf-idf matrix, sort data according to frequency and create a data table with terms and frequency of occurrence for each n-gram
```{r}
# Uni-Gram
businessData.uniGramDTM = generateTermDocumentMatrix(stemmedTrainData, 1)
businessData.uniGramDTM.matrix <- as.matrix(businessData.uniGramDTM)
businessData.uniGramDTMTfIdf <- calculateTfidf(businessData.uniGramDTM.matrix)
freq1 <- sort(rowSums(businessData.uniGramDTMTfIdf), decreasing=TRUE)
wf1 <- data.table(term=names(freq1), Freq=freq1)
# Generate transpose of TDM as LDA accepts only DTM
businessData.uniGramDTMTfIdf <- t(businessData.uniGramDTMTfIdf)
generateWordCloud(businessData.uniGramDTMTfIdf, 45)
generateDFPlot(businessData.uniGramDTMTfIdf, "Uni-Gram", "Uni-Gram vs Frequency (Top 10)")


# Bi-Gram
businessData.biGramDTM = generateTermDocumentMatrix(stemmedTrainData, 2)
businessData.biGramDTM.matrix <- as.matrix(businessData.biGramDTM)
businessData.biGramDTMTfIdf <- calculateTfidf(businessData.biGramDTM.matrix)
freq2 <- sort(rowSums(businessData.biGramDTMTfIdf), decreasing=TRUE)
biGramfirstTerms <- firstWords(names(freq2))
biGramlastTerms <- lastWord(names((freq2)))
wf2 <- data.table(word1=biGramfirstTerms, nextWord=biGramlastTerms, Freq=freq2)
# Generate transpose of TDM as LDA accepts only DTM
businessData.biGramDTMTfIdf <- t(businessData.biGramDTMTfIdf)
generateWordCloud(businessData.biGramDTMTfIdf, 9.5)
generateDFPlot(businessData.biGramDTMTfIdf, "Bi-Gram", "Bi-Gram vs Frequency (Top 10)")


# Tri-Gram
businessData.triGramDTM= generateTermDocumentMatrix(stemmedTrainData, 3)
businessData.triGramDTM.matrix <- as.matrix(businessData.triGramDTM)
businessData.triGramDTMTfIdf <- calculateTfidf(businessData.triGramDTM.matrix)
freq3 <- sort(rowSums(businessData.triGramDTMTfIdf), decreasing=TRUE)
triGramfirstTerms <- firstWords(names(freq3))
triGramlastTerms <- lastWord(names((freq3)))
triGramTemp <- data.table(words=triGramfirstTerms, w=triGramlastTerms, Freq=freq3)
triGramTemp[, c("w1", "w2") := tstrsplit(words, " ", fixed=TRUE)]
wf3 <- data.table(word1=triGramTemp$w1, word2=triGramTemp$w2, nextWord=triGramTemp$w, Freq=triGramTemp$Freq)
# Generate transpose of TDM as LDA accepts only DTM
businessData.triGramDTMTfIdf <- t(businessData.triGramDTMTfIdf)
generateWordCloud(businessData.triGramDTMTfIdf, 9)
generateDFPlot(businessData.triGramDTMTfIdf, "Tri-Gram", "Tri-Gram vs Frequency (Top 10)")


# Quad-Gram
businessData.quadGramDTM= generateTermDocumentMatrix(stemmedTrainData, 4)
businessData.quadGramDTM.matrix <- as.matrix(businessData.quadGramDTM)
businessData.quadGramDTMTfIdf <- calculateTfidf(businessData.quadGramDTM.matrix)
freq4 <- sort(rowSums(businessData.quadGramDTMTfIdf), decreasing=TRUE)
quadGramfirstTerms <- firstWords(names(freq4))
quadGramlastTerms <- lastWord(names((freq4)))
quadGramTemp <- data.table(words=quadGramfirstTerms, w=quadGramlastTerms, Freq=freq4)
quadGramTemp[, c("w1", "w2", "w3") := tstrsplit(words, " ", fixed=TRUE)]
wf4 <- data.table(word1=quadGramTemp$w1, word2=quadGramTemp$w2, word3=quadGramTemp$w3, nextWord=quadGramTemp$w, Freq=quadGramTemp$Freq)
# Generate transpose of TDM as LDA accepts only DTM
businessData.quadGramDTMTfIdf <- t(businessData.quadGramDTMTfIdf)
generateWordCloud(businessData.quadGramDTMTfIdf, 6)
generateDFPlot(businessData.quadGramDTMTfIdf, "Quad-Gram", "Quad-Gram vs Frequency (Top 10)")
```


#### <br/><br/> Bar-plot to depict the number of n-grams present in the data
```{r}
#Barplot for NGram Count
nGram.count <- c(ncol(businessData.uniGramDTMTfIdf),ncol(businessData.biGramDTMTfIdf),ncol(businessData.triGramDTMTfIdf), ncol(businessData.quadGramDTMTfIdf))
nGram.name <- c('Uni-Gram', 'Bi-Gram', 'Tri-Gram', 'Quad-Gram')
countDF <- data.frame(nGram.name, nGram.count)
plot_ly(countDF, x=~nGram.name, y=~nGram.count, type="bar", name="Number of N-Grams vs N-Gram size", marker = list(color =c("Orange", "Lightblue", "Lightgreen", 'Gray') ), color=~nGram.name)%>%
  layout(yaxis = list(title = 'Count'), xaxis=list(title='N-Grams'), showlegend=TRUE)
```


#### <br/><br/>KN Smoothing for each n-gram. Generates smoothed data with term, nextTerm and frequency of occurrence together. Top 10 terms are listed for each n-gram.
```{r}
# Uni-Gram
sumFreq <- sum(wf1$Freq)
probab = c()
for(i in 1:nrow(wf1)){
  probab[i] = wf1$Freq[i]/sumFreq
}
uniGramSmoothedData <- data.table(wf1$term, wf1$Freq, probab)
names(uniGramSmoothedData) <- c("term", "frequency", "probability")
head(uniGramSmoothedData, 10)

# Bi-Gram
biGram <- KNSmoothing(wf2, 2)
biGramSmoothedData <- data.table(biGram$word1, biGram$nextWord, biGram$Freq, biGram$P_KN)
names(biGramSmoothedData) <- c("term", "nextTerm", "frequency", "probability")
head(biGramSmoothedData, 10)

# Tri-Gram
triGram <- KNSmoothing(wf3, 3)
triGramSmoothedData <- data.table(paste(triGram$word1, triGram$word2, sep=" "), triGram$nextWord, triGram$Freq, triGram$P_KN)
names(triGramSmoothedData) <- c("term", "nextTerm", "frequency", "probability")
head(triGramSmoothedData, 10)

# Quad-Gram
quadGram <- KNSmoothing(wf4, 4)
quadGramSmoothedData <- data.table(paste(quadGram$word1, quadGram$word2, quadGram$word3, sep=" "), quadGram$nextWord, quadGram$Freq, quadGram$P_KN)
names(quadGramSmoothedData) <- c("term", "nextTerm", "frequency", "probability")
head(quadGramSmoothedData, 10)
```


#### <br/><br/>Generate LDA matrix for each n-gram
```{r}
buisness.uniGram.lda <- generateTopicModellingUsingLDA(businessData.uniGramDTMTfIdf,10)
buisness.biGram.lda <- generateTopicModellingUsingLDA(as.matrix(businessData.biGramDTMTfIdf),10)
buisness.triGram.lda <- generateTopicModellingUsingLDA(businessData.triGramDTMTfIdf,10)
buisness.quadGram.lda <- generateTopicModellingUsingLDA(businessData.quadGramDTMTfIdf,10)
```

#### <br/><br/>Output from LDA (only top 10 values are shown below)
```{r}
showTopicProbabilities(buisness.uniGram.lda@gamma)
showTopicProbabilities(buisness.biGram.lda@gamma)
showTopicProbabilities(buisness.triGram.lda@gamma)
showTopicProbabilities(buisness.quadGram.lda@gamma)
```


#### <br/><br/>Generate top 10 terms under each topic
```{r}
generateTopNTermsInTopic(buisness.uniGram.lda, 10)
generateTopNTermsInTopic(buisness.biGram.lda, 10)
generateTopNTermsInTopic(buisness.triGram.lda, 10)
generateTopNTermsInTopic(buisness.quadGram.lda, 10)
```

#### <br/><br/>Calculate mean topic probabilities across all documents
```{r}
uni.topicMeanProbabDF <- calculateTopicProbabilityAcrossAllDocuments(buisness.uniGram.lda@gamma)
bi.topicMeanProbabDF <- calculateTopicProbabilityAcrossAllDocuments(buisness.biGram.lda@gamma)
tri.topicMeanProbabDF <- calculateTopicProbabilityAcrossAllDocuments(buisness.triGram.lda@gamma)
quad.topicMeanProbabDF <- calculateTopicProbabilityAcrossAllDocuments(buisness.quadGram.lda@gamma)
```

#### <br/><br/>Assign topic probability to LDA top N terms
```{r}
uni.termLDAProbability <- assignTermLdaProbability(buisness.uniGram.lda,uni.topicMeanProbabDF)
bi.termLDAProbability <- assignTermLdaProbability(buisness.biGram.lda,bi.topicMeanProbabDF)
tri.termLDAProbability <- assignTermLdaProbability(buisness.triGram.lda,tri.topicMeanProbabDF)
quad.termLDAProbability <- assignTermLdaProbability(buisness.quadGram.lda,quad.topicMeanProbabDF)
```

#### <br/><br/>Calculate mean probability for each term 
```{r}
uni.combinedProb <- mergeProbability(uni.termLDAProbability,uniGramSmoothedData)
names(uni.combinedProb) <- c("nextTerm", "frequency", "smoothedProbability","ldaProbability","probability")

bi.combinedProb <- mergeProbability(bi.termLDAProbability,biGramSmoothedData)
names(bi.combinedProb) <- c("term","nextTerm","frequency", "smoothedProbability","wholeTerm","ldaProbability","probability")

tri.combinedProb <- mergeProbability(tri.termLDAProbability,triGramSmoothedData)
names(tri.combinedProb) <- c("term","nextTerm","frequency", "smoothedProbability","wholeTerm","ldaProbability","probability")

quad.combinedProb <- mergeProbability(quad.termLDAProbability,quadGramSmoothedData)
names(quad.combinedProb) <- c("term","nextTerm","frequency", "smoothedProbability","wholeTerm","ldaProbability","probability")
```

#### <br/><br/>Evaluation (Jaccard Similarity Index)
```{r}
# Predicted text from Auto-complete tool
predictedText = "My wife had chicken tenders that was extremely tasty.Tonight the restaurant was almost fully crowded."
doc = c(1:length(testData$text))
similarity_index = c()
for(i in 1 : length(testData$text)) {
  eachRow <- testData$text[i]
  tdm <- generateTermDocumentMatrix(c(predictedText,eachRow),1)
  tdm.matrix <- as.matrix(tdm)
  jaccard <- cluster_similarity(tdm.matrix[,1], tdm.matrix[,2], similarity = "jaccard",method = "independence")
  cat("Jaccard similarity between two documents ",jaccard,"\n")
  similarity_index[i] = jaccard
}
evaluationDF <- data.frame(doc, similarity_index)
names(evaluationDF) <- c("Document", "Value")
plot_ly(evaluationDF, x=~doc, y=~similarity_index, type="bar", name="Jaccard Similarity Index", color=~Document)%>%
  layout(yaxis = list(title = 'Similarity Index'), xaxis=list(title='Document'), showlegend=TRUE)
```


#### <br/><br/>Sample output during prediction
```{r}
predictionGiven0Word("my wife ordered")
predictionGiven1Word("my wife ordered")
predictionGiven2Words("my wife")
predictionGiven3Words("my wife ordered")
```


#### <br/><br/><br/><br/>



